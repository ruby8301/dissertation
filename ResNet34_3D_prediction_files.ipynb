{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1befc33e",
   "metadata": {},
   "source": [
    "# ResNet34-3D Inference → CSV (train/val)\n",
    "Follow the steps below to produce cnn_preds/resnet_train.csv and cnn_preds/resnet_val.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4bfa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT = C:\\Users\\rubia\\Downloads\n",
      "{'train': WindowsPath('C:/Users/rubia/Downloads/train'), 'val': WindowsPath('C:/Users/rubia/Downloads/val'), 'test': WindowsPath('C:/Users/rubia/Downloads/test')}\n",
      "TRAIN_CASES_CSV: C:\\Users\\rubia\\Downloads\\train_cases (2).csv\n",
      "VAL_CASES_CSV: C:\\Users\\rubia\\Downloads\\val_cases.csv\n",
      "CNN_PREDS_DIR: C:\\Users\\rubia\\Downloads\\cnn_preds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path(r\"C:\\\\Users\\\\rubia\\\\Downloads\").resolve()\n",
    "print(\"DATA_ROOT =\", DATA_ROOT)\n",
    "SPLIT_DIRS = {\"train\": DATA_ROOT/\"train\", \"val\": DATA_ROOT/\"val\", \"test\": DATA_ROOT/\"test\"}\n",
    "print(SPLIT_DIRS)\n",
    "TRAIN_CASES_CSV = DATA_ROOT/\"train_cases (2).csv\"\n",
    "VAL_CASES_CSV = DATA_ROOT/\"val_cases.csv\" if (DATA_ROOT/\"val_cases.csv\").exists() else DATA_ROOT/\"val_cases (2).csv\"\n",
    "CNN_PREDS_DIR = DATA_ROOT/\"cnn_preds\"; CNN_PREDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"TRAIN_CASES_CSV:\", TRAIN_CASES_CSV)\n",
    "print(\"VAL_CASES_CSV:\", VAL_CASES_CSV)\n",
    "print(\"CNN_PREDS_DIR:\", CNN_PREDS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb0cb08-a835-43d1-8f5e-369fa8e1bc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\rubia\\Downloads\\train_cases.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(r\"C:\\Users\\rubia\\Downloads\")\n",
    "\n",
    "src = DATA_ROOT / \"train_cases (2).csv\"   # your current file\n",
    "dst = DATA_ROOT / \"train_cases.csv\"       # clean target\n",
    "\n",
    "df = pd.read_csv(src, header=None, engine=\"python\", sep=None)  # auto-detect delim\n",
    "df = df.iloc[:, :2]                    # keep first two columns\n",
    "df.columns = [\"id\", \"label\"]           # add headers\n",
    "df.to_csv(dst, index=False, encoding=\"utf-8\")  # write clean (no BOM)\n",
    "print(\"Wrote:\", dst)\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(r\"C:\\Users\\rubia\\Downloads\").resolve()\n",
    "TRAIN_CASES_CSV = DATA_ROOT / \"train_cases.csv\"  # now guaranteed to have id,label\n",
    "VAL_CASES_CSV   = DATA_ROOT / \"val_cases.csv\" if (DATA_ROOT / \"val_cases.csv\").exists() else DATA_ROOT / \"val_cases (2).csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e235787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "try:\n",
    "    import nibabel as nib\n",
    "except Exception as e:\n",
    "    raise ImportError(\"nibabel is required. Install with: pip install nibabel\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "assert (DATA_ROOT/\"train\").exists(), \"Missing train folder\"\n",
    "assert (DATA_ROOT/\"val\").exists(), \"Missing val folder\"\n",
    "assert (TRAIN_CASES_CSV).exists(), \"Missing train_cases.csv\"\n",
    "assert (VAL_CASES_CSV).exists(), f\"Missing val_cases.csv at {VAL_CASES_CSV}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549a9676-9eca-4dc6-bf87-e84f6e70f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; pd.read_csv(r\"C:\\Users\\rubia\\Downloads\\train_cases (2).csv\", header=None).iloc[:, :2].to_csv(r\"C:\\Users\\rubia\\Downloads\\train_cases.csv\", index=False, header=[\"id\",\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb52a96-6677-44f4-a510-5798ec63da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights: C:\\Users\\rubia\\Downloads\\resnet_34.pth\n",
      "[3D loader] Missing keys (first 10): ['layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var']\n",
      "Model expects in_channels: 1\n",
      "[OK] Saved train predictions → C:\\Users\\rubia\\Downloads\\cnn_preds\\resnet_train.csv\n",
      "[OK] Saved val predictions → C:\\Users\\rubia\\Downloads\\cnn_preds\\resnet_val.csv\n"
     ]
    }
   ],
   "source": [
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "class BasicBlock3D(nn.Module):\n",
    "    expansion=1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1=conv3x3x3(inplanes, planes, stride); self.bn1=nn.BatchNorm3d(planes); self.relu=nn.ReLU(inplace=True)\n",
    "        self.conv2=conv3x3x3(planes, planes); self.bn2=nn.BatchNorm3d(planes); self.downsample=downsample\n",
    "    def forward(self,x):\n",
    "        identity=x; out=self.relu(self.bn1(self.conv1(x))); out=self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None: identity=self.downsample(x)\n",
    "        out+=identity; return self.relu(out)\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=2, in_channels=1):\n",
    "        super().__init__(); self.inplanes=64\n",
    "        self.conv1=nn.Conv3d(in_channels,64,kernel_size=7,stride=(2,2,2),padding=3,bias=False); self.bn1=nn.BatchNorm3d(64); self.relu=nn.ReLU(inplace=True)\n",
    "        self.maxpool=nn.MaxPool3d(kernel_size=3,stride=2,padding=1)\n",
    "        self.layer1=self._make_layer(block,64,layers[0]); self.layer2=self._make_layer(block,128,layers[1],stride=2)\n",
    "        self.layer3=self._make_layer(block,256,layers[2],stride=2); self.layer4=self._make_layer(block,512,layers[3],stride=2)\n",
    "        self.avgpool=nn.AdaptiveAvgPool3d((1,1,1)); self.fc=nn.Linear(512*block.expansion,num_classes)\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        down=None\n",
    "        if stride!=1 or self.inplanes!=planes*block.expansion:\n",
    "            down=nn.Sequential(nn.Conv3d(self.inplanes,planes*block.expansion,kernel_size=1,stride=stride,bias=False), nn.BatchNorm3d(planes*block.expansion))\n",
    "        layers=[block(self.inplanes,planes,stride,down)]; self.inplanes=planes*block.expansion\n",
    "        for _ in range(1,blocks): layers.append(block(self.inplanes,planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x); x=self.bn1(x); x=self.relu(x); x=self.maxpool(x)\n",
    "        x=self.layer1(x); x=self.layer2(x); x=self.layer3(x); x=self.layer4(x)\n",
    "        x=self.avgpool(x); x=torch.flatten(x,1); return self.fc(x)\n",
    "def resnet34_3d(num_classes=2, in_channels=1):\n",
    "    return ResNet3D(BasicBlock3D, [3,4,6,3], num_classes=num_classes, in_channels=in_channels)\n",
    "\n",
    "\n",
    "def _unwrap_state_dict(ckpt):\n",
    "    if isinstance(ckpt, dict) and 'state_dict' in ckpt: sd=ckpt['state_dict']\n",
    "    elif isinstance(ckpt, dict): sd=ckpt\n",
    "    else: raise ValueError('Unexpected checkpoint format')\n",
    "    if any(k.startswith('module.') for k in sd.keys()): sd={k.replace('module.','',1):v for k,v in sd.items()}\n",
    "    return sd\n",
    "def _infer_shapes_from_sd(sd):\n",
    "    in_ch=1; ncls=2\n",
    "    w=sd.get('conv1.weight');\n",
    "    if isinstance(w, torch.Tensor) and w.ndim==5: in_ch=int(w.shape[1])\n",
    "    fcw=sd.get('fc.weight');\n",
    "    if isinstance(fcw, torch.Tensor) and fcw.ndim==2: ncls=int(fcw.shape[0])\n",
    "    return in_ch, ncls\n",
    "def load_resnet34_3d(weights_path: Path):\n",
    "    try:\n",
    "        ckpt=torch.load(weights_path,map_location='cpu',weights_only=True)\n",
    "    except TypeError:\n",
    "        ckpt=torch.load(weights_path,map_location='cpu')\n",
    "    sd=_unwrap_state_dict(ckpt); in_ch,ncls=_infer_shapes_from_sd(sd)\n",
    "    m=resnet34_3d(num_classes=ncls, in_channels=in_ch)\n",
    "    missing,unexpected=m.load_state_dict(sd, strict=False)\n",
    "    if missing: print('[3D loader] Missing keys (first 10):', missing[:10])\n",
    "    if unexpected: print('[3D loader] Unexpected keys (first 10):', unexpected[:10])\n",
    "    return m.to(DEVICE).eval()\n",
    "\n",
    "\n",
    "CANDIDATE_FILENAMES = [\n",
    "    'NM.nii.gz','NM.nii','nm.nii.gz','nm.nii',\n",
    "    'T1.nii.gz','T1.nii','t1.nii.gz','t1.nii',\n",
    "    'QSM.nii.gz','QSM.nii','qsm.nii.gz','qsm.nii',\n",
    "]\n",
    "def _list_dir(p: Path, max_items=10):\n",
    "    try: items=sorted([f.name for f in p.iterdir()]); return items[:max_items]+(['...'] if len(items)>max_items else [])\n",
    "    except Exception: return []\n",
    "def _find_nifti_for_subject(base: Path, subject_id: str) -> Path:\n",
    "    d=base/subject_id\n",
    "    if not d.exists():\n",
    "        raise FileNotFoundError(f'Subject folder not found: {d}\\nExisting in {base} (first 10): {_list_dir(base)}')\n",
    "    for n in CANDIDATE_FILENAMES:\n",
    "        p=d/n\n",
    "        if p.exists(): return p\n",
    "    nii=list(d.glob('*.nii'))+list(d.glob('*.nii.gz'))\n",
    "    if nii: return nii[0]\n",
    "    raise FileNotFoundError(f'No NIfTI found in {d}. Tried known names and *.nii/*.nii.gz. Contents: {_list_dir(d)}')\n",
    "class VolumeDataset(Dataset):\n",
    "    def __init__(self, csv_path: Path, split_name: str, target_size=(128,128,128)):\n",
    "        self.df = pd.read_csv(csv_path, engine=\"python\", sep=None)\n",
    "        self.df.columns = [c.strip().lower().replace(\"\\ufeff\",\"\") for c in self.df.columns]  # strip BOM/whitespace        \n",
    "        cols=[c.lower() for c in self.df.columns]\n",
    "        if 'subject_id' not in self.df.columns:\n",
    "            if 'id' in cols: self.df=self.df.rename(columns={self.df.columns[cols.index('id')]: 'subject_id'})\n",
    "            else: raise ValueError(\"CSV must have 'subject_id' or 'id' column.\")\n",
    "        self.base=SPLIT_DIRS[split_name]; self.target_size=target_size\n",
    "        mask=self.df['subject_id'].astype(str).apply(lambda sid: (self.base/sid).exists())\n",
    "        missing=self.df.loc[~mask,'subject_id'].astype(str).tolist()\n",
    "        if missing: print(f'[WARN] {len(missing)} subjects missing under {self.base}. Example(s): {missing[:5]}')\n",
    "        self.df=self.df.loc[mask].reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sid=str(self.df.iloc[idx]['subject_id']); p=_find_nifti_for_subject(self.base,sid)\n",
    "        vol=nib.load(str(p)).get_fdata().astype(np.float32)\n",
    "        m,s=float(vol.mean()), float(vol.std()) or 1.0; vol=(vol-m)/s\n",
    "        t=torch.from_numpy(vol).float()\n",
    "        if t.ndim!=3: raise ValueError(f'Expected 3D volume, got {tuple(t.shape)} for {p}')\n",
    "        t=t.unsqueeze(0).unsqueeze(0)\n",
    "        t=F.interpolate(t, size=self.target_size, mode='trilinear', align_corners=False)\n",
    "        t=t.squeeze(0) # [C=1, D,H,W]\n",
    "        return sid, t\n",
    "\n",
    "\n",
    "weights_path = DATA_ROOT/'resnet_34.pth'\n",
    "if not weights_path.exists():\n",
    "    env_p=os.getenv('RESNET34_WEIGHTS')\n",
    "    if env_p: weights_path=Path(env_p)\n",
    "\n",
    "print('Using weights:', weights_path)\n",
    "model=load_resnet34_3d(weights_path)\n",
    "expected_in=model.conv1.in_channels\n",
    "print('Model expects in_channels:', expected_in)\n",
    "@torch.no_grad()\n",
    "def infer_split(csv_path: Path, split_name: str, out_csv: Path, batch=1):\n",
    "    ds=VolumeDataset(csv_path, split_name)\n",
    "    dl=DataLoader(ds, batch_size=batch, shuffle=False, num_workers=0)\n",
    "    rows=[]\n",
    "    for sids, vols in dl:\n",
    "        if vols.shape[1]!=expected_in: vols=vols.repeat(1, expected_in, 1,1,1)\n",
    "        vols=vols.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        logits=model(vols)\n",
    "        probs=torch.sigmoid(logits[:,0]) if logits.shape[1]==1 else torch.softmax(logits,dim=1)[:,1]\n",
    "        rows += [{\"subject_id\": sid, \"resnet_prob\": float(p)} for sid,p in zip(sids, probs.cpu().numpy().tolist())]\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "    print(f'[OK] Saved {split_name} predictions → {out_csv}')\n",
    "if TRAIN_CASES_CSV.exists(): infer_split(TRAIN_CASES_CSV, 'train', CNN_PREDS_DIR/'resnet_train.csv', batch=1)\n",
    "else: print('[Skip] TRAIN_CASES_CSV not found:', TRAIN_CASES_CSV)\n",
    "if VAL_CASES_CSV.exists(): infer_split(VAL_CASES_CSV, 'val', CNN_PREDS_DIR/'resnet_val.csv', batch=1)\n",
    "else: print('[Skip] VAL_CASES_CSV not found:', VAL_CASES_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65eb70b-e4f2-4985-90be-bed95583ebc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
